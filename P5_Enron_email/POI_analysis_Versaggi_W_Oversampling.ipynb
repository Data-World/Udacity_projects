{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "# Inline Graphics for IPython NB's\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "features_list = ['poi','bonus', 'deferred_income','exercised_stock_options', 'expenses','loan_advances',\n",
    "                 'long_term_incentive','other','restricted_stock','restricted_stock_deferred','salary',\n",
    "                 'total_payments','total_stock_value', 'deferral_payments', 'director_fees', 'shared_receipt_with_poi'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the (pickle) dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonus                        float64\n",
      "deferral_payments            float64\n",
      "deferred_income              float64\n",
      "director_fees                float64\n",
      "email_address                  int64\n",
      "exercised_stock_options      float64\n",
      "expenses                     float64\n",
      "from_messages                float64\n",
      "from_poi_to_this_person      float64\n",
      "from_this_person_to_poi      float64\n",
      "loan_advances                float64\n",
      "long_term_incentive          float64\n",
      "other                        float64\n",
      "poi                            int64\n",
      "restricted_stock             float64\n",
      "restricted_stock_deferred    float64\n",
      "salary                       float64\n",
      "shared_receipt_with_poi      float64\n",
      "to_messages                  float64\n",
      "total_payments               float64\n",
      "total_stock_value            float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "# Convert the Python Dictionary to a Pandas DataFrame:\n",
    "raw_dataframe = pd.DataFrame(data_dict)\n",
    "raw_dataframe.head(5)\n",
    "dataset = raw_dataframe.T\n",
    "mydataset = dataset.convert_objects(convert_dates=True, convert_numeric=True, convert_timedeltas=True, copy=True)\n",
    "\n",
    "mydataset['email_address'] = mydataset['email_address'].apply(lambda x: 0 if x == 'NaN' else 1)\n",
    "mydataset = mydataset.fillna(0)\n",
    "\n",
    "mydataset = mydataset.abs()\n",
    "mydataset.drop(mydataset.index[130], inplace=True)\n",
    "mydataset.drop(mydataset.index[127], inplace=True)\n",
    "mydataset['poi'] = mydataset['poi'].astype(int)\n",
    "\n",
    "mydataset.head(5)\n",
    "mydataset = mydataset.convert_objects(convert_dates=True, convert_numeric=True, convert_timedeltas=True, copy=True)\n",
    "print(mydataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_mydataset = mydataset.copy(deep=True)\n",
    "MyPoiDF = mydataset.loc[mydataset['poi'] == 1].copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do this ONLY ONCE per oversampling run to initialize.\n",
    "start_range = 0\n",
    "# CHANGE the INDEX of the DF to a numeric range,\n",
    "# this can be done multiple times in sequence to add to the oversampling.\n",
    "\n",
    "# Calculate Proper Range Values for this individual DF\n",
    "end_range = len(MyPoiDF)\n",
    "\n",
    "# RESET the Temp POI DF (for unique indexes)\n",
    "MyPoiDF.index=range(start_range, end_range + start_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "over_sampled_df = temp_mydataset.append(MyPoiDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple Model\n",
    "clf = SVC(kernel='rbf')                                              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverted_df = over_sampled_df.T\n",
    "reverted_dict = reverted_df.to_dict()\n",
    "my_dataset = reverted_dict\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "        \n",
    "        \n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "        \n",
    "        \n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision,recall =  1.0 0.9155\n",
      "\tAccuracy: 0.98012\tPrecision: 1.00000\tRecall: 0.91550\tF1: 0.95589\tF2: 0.93124\n",
      "\tTotal predictions: 17000\tTrue positives: 3662\tFalse positives:    0\tFalse negatives:  338\tTrue negatives: 13000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "features_list = ['poi','bonus','deferral_payments','deferred_income','director_fees',\n",
    "                                   'exercised_stock_options','expenses','loan_advances','long_term_incentive',\n",
    "                                   'other','restricted_stock','restricted_stock_deferred','salary',\n",
    "                                   'shared_receipt_with_poi','total_payments','total_stock_value']\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Precision and recall above .9 for 1 X oversampling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
